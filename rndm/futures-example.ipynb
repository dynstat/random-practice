{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futures in Python: From Basic to Advanced\n",
    "\n",
    "## 1. Introduction to Futures\n",
    "\n",
    "Futures in Python represent the result of asynchronous computations. They are a powerful tool for managing concurrent operations and can significantly improve the performance of I/O-bound and CPU-bound tasks.\n",
    "\n",
    "Analogy: Think of a future as a 'promise' or an 'IOU' (I Owe You) note. When you start a task, you get a future object immediately, which is like receiving an IOU. You can continue with other work, and when you need the result, you can 'cash in' the IOU to get the actual value.\n",
    "\n",
    "Let's start with a simple example to illustrate the concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def slow_operation(seconds):\n",
    "    time.sleep(seconds)  # Simulate a time-consuming operation\n",
    "    return f\"Operation completed in {seconds} seconds\"\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the task and get a future\n",
    "    future = executor.submit(slow_operation, 3)\n",
    "    \n",
    "    print(\"Future created. Doing other work...\")\n",
    "    time.sleep(1)  # Simulate other work\n",
    "    \n",
    "    # Get the result from the future\n",
    "    result = future.result()\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. We define a `slow_operation` function that simulates a time-consuming task.\n",
    "2. We create a `ThreadPoolExecutor` to manage concurrent operations.\n",
    "3. We submit our task to the executor, which immediately returns a future object.\n",
    "4. We can do other work while the future is being processed.\n",
    "5. When we need the result, we call `future.result()`, which will wait if necessary and then return the result.\n",
    "\n",
    "This demonstrates the basic concept of futures: they allow us to start a task and continue with other work, checking for the result when we need it.\n",
    "\n",
    "## 2. Basics of Asyncio\n",
    "\n",
    "Before diving deeper into futures, it's important to understand the basics of asyncio, as it's closely related to how futures work in Python.\n",
    "\n",
    "Asyncio is a library for writing concurrent code using the async/await syntax. It's particularly useful for I/O-bound and high-level structured network code.\n",
    "\n",
    "Analogy: Think of asyncio as a juggler. A juggler can keep multiple balls in the air by quickly switching attention between them. Similarly, asyncio can manage multiple tasks by switching between them when they're waiting for I/O operations.\n",
    "\n",
    "Let's see a basic asyncio example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def say_after(delay, what):\n",
    "    await asyncio.sleep(delay)\n",
    "    print(what)\n",
    "\n",
    "async def main():\n",
    "    print(\"started at\", time.strftime(\"%X\"))\n",
    "\n",
    "    await say_after(1, 'hello')\n",
    "    await say_after(2, 'world')\n",
    "\n",
    "    print(\"finished at\", time.strftime(\"%X\"))\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. We define an asynchronous function `say_after` that waits for a specified delay and then prints a message.\n",
    "2. Our `main` function calls `say_after` twice, waiting for each to complete before moving on.\n",
    "3. We use `asyncio.run()` to run our main coroutine.\n",
    "\n",
    "This is a synchronous use of asyncio. In the next section, we'll see how to use futures with asyncio for concurrent operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concurrent Futures\n",
    "\n",
    "The `concurrent.futures` module provides a high-level interface for asynchronously executing callables. It abstracts the complexities of using threads or processes, making it easier to write concurrent code.\n",
    "\n",
    "Analogy: Think of `concurrent.futures` as a team of workers. You can assign tasks to this team, and they'll work on them concurrently. You can check on the progress of any task at any time, or wait for all tasks to complete.\n",
    "\n",
    "Let's explore the two main types of executors in `concurrent.futures`: ThreadPoolExecutor and ProcessPoolExecutor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ThreadPoolExecutor\n",
    "\n",
    "ThreadPoolExecutor is used for I/O-bound tasks. It's suitable when your tasks spend a lot of time waiting for external operations (like network requests or file I/O).\n",
    "\n",
    "Let's see an example where we use ThreadPoolExecutor to download multiple web pages concurrently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def download_page(url):\n",
    "    response = requests.get(url)\n",
    "    return f\"Downloaded {url}, status: {response.status_code}, length: {len(response.text)}\"\n",
    "\n",
    "urls = [\n",
    "    'https://www.example.com',\n",
    "    'https://www.python.org',\n",
    "    'https://www.github.com',\n",
    "    'https://www.stackoverflow.com'\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit all tasks and create a map of futures to their URLs\n",
    "    future_to_url = {executor.submit(download_page, url): url for url in urls}\n",
    "    \n",
    "    # Iterate over the futures as they complete\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            data = future.result()\n",
    "        except Exception as exc:\n",
    "            print(f\"{url} generated an exception: {exc}\")\n",
    "        else:\n",
    "            print(data)\n",
    "\n",
    "print(f\"Total time taken: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this example:\n",
    "\n",
    "1. We define a `download_page` function that downloads a webpage and returns a string with information about the download.\n",
    "\n",
    "2. We create a list of URLs to download.\n",
    "\n",
    "3. We use a `ThreadPoolExecutor` with a maximum of 4 workers. This means up to 4 downloads can happen concurrently.\n",
    "\n",
    "4. We submit all tasks to the executor using a dictionary comprehension. This creates a mapping of Future objects to their corresponding URLs.\n",
    "\n",
    "5. We use `concurrent.futures.as_completed()` to iterate over the futures as they complete. This allows us to process results as soon as they're available, rather than waiting for all tasks to finish.\n",
    "\n",
    "6. For each completed future, we retrieve its result (or catch any exceptions), and print the output.\n",
    "\n",
    "7. Finally, we print the total time taken for all downloads.\n",
    "\n",
    "This example demonstrates how ThreadPoolExecutor can significantly speed up I/O-bound operations by performing them concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ProcessPoolExecutor\n",
    "\n",
    "While ThreadPoolExecutor is great for I/O-bound tasks, ProcessPoolExecutor is better suited for CPU-bound tasks. It uses separate processes instead of threads, which allows it to bypass the Global Interpreter Lock (GIL) and achieve true parallelism on multi-core systems.\n",
    "\n",
    "Let's see an example where we use ProcessPoolExecutor to perform a CPU-intensive task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "import time\n",
    "\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(math.sqrt(n)) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def count_primes(start, end):\n",
    "    return sum(1 for n in range(start, end) if is_prime(n))\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the range to search for primes\n",
    "    start, end = 1, 10**7\n",
    "    \n",
    "    # Split the range into chunks for each process\n",
    "    chunk_size = (end - start) // 4\n",
    "    ranges = [(i, i + chunk_size) for i in range(start, end, chunk_size)]\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        results = executor.map(lambda r: count_primes(*r), ranges)\n",
    "    \n",
    "    total_primes = sum(results)\n",
    "    \n",
    "    print(f\"Found {total_primes} prime numbers in range {start} to {end}\")\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this example:\n",
    "\n",
    "1. We define an `is_prime` function to check if a number is prime.\n",
    "\n",
    "2. We define a `count_primes` function that counts the number of primes in a given range.\n",
    "\n",
    "3. In the `main` function:\n",
    "   - We set the range to search for primes (1 to 10^7).\n",
    "   - We split this range into 4 chunks, one for each process.\n",
    "\n",
    "4. We use a `ProcessPoolExecutor` to distribute the work across multiple processes.\n",
    "\n",
    "5. We use `executor.map()` to apply our `count_primes` function to each range. This returns an iterator of results.\n",
    "\n",
    "6. We sum up the results to get the total number of primes found.\n",
    "\n",
    "7. Finally, we print the results and the time taken.\n",
    "\n",
    "This example shows how ProcessPoolExecutor can be used to parallelize CPU-intensive tasks across multiple cores, potentially providing significant speedup on multi-core systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Futures: Internal Flow and Analogies\n",
    "\n",
    "To understand how futures work internally, let's use an analogy of a coffee shop. In this analogy:\n",
    "- The main program is the cashier\n",
    "- The ThreadPoolExecutor is the team of baristas\n",
    "- Each future is an order ticket\n",
    "- The tasks are different coffee orders\n",
    "\n",
    "Here's a diagram to visualize this:\n",
    "\n",
    "```\n",
    "                 +-------------+\n",
    "                 |   Cashier   |\n",
    "                 | (Main Thread) |\n",
    "                 +-------------+\n",
    "                        |\n",
    "                        | Submit orders\n",
    "                        v\n",
    "            +------------------------+\n",
    "            |    ThreadPoolExecutor  |\n",
    "            | (Team of Baristas)     |\n",
    "            +------------------------+\n",
    "                 |       |       |\n",
    "            +----+   +---+   +---+\n",
    "            |        |       |\n",
    "         Future1  Future2  Future3\n",
    "         (Order1) (Order2) (Order3)\n",
    "```\n",
    "\n",
    "Let's implement this analogy in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import random\n",
    "\n",
    "def make_coffee(order):\n",
    "    # Simulate coffee making process\n",
    "    print(f\"Starting to make {order}\")\n",
    "    time.sleep(random.uniform(1, 3))  # Random time to make coffee\n",
    "    return f\"{order} is ready!\"\n",
    "\n",
    "def coffee_shop():\n",
    "    orders = ['Espresso', 'Latte', 'Cappuccino', 'Americano']\n",
    "    \n",
    "    print(\"Coffee shop is open!\")\n",
    "    \n",
    "    # Create a ThreadPoolExecutor with 2 workers (baristas)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as barista_team:\n",
    "        # Submit all orders and create a map of futures to their orders\n",
    "        future_to_order = {barista_team.submit(make_coffee, order): order for order in orders}\n",
    "        \n",
    "        print(\"All orders submitted to baristas. Cashier is free now.\")\n",
    "        \n",
    "        # Simulate cashier doing other work\n",
    "        time.sleep(1)\n",
    "        print(\"Cashier is checking the status of orders...\")\n",
    "        \n",
    "        # Check and collect finished orders\n",
    "        for future in concurrent.futures.as_completed(future_to_order):\n",
    "            order = future_to_order[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"{order} order failed: {e}\")\n",
    "            else:\n",
    "                print(f\"Cashier: {result}\")\n",
    "    \n",
    "    print(\"Coffee shop is closed!\")\n",
    "\n",
    "coffee_shop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this code and explain how it relates to our coffee shop analogy:\n",
    "\n",
    "1. `make_coffee(order)`: This function represents a barista making a coffee. It simulates the time taken to make a coffee using `time.sleep()`.\n",
    "\n",
    "2. `coffee_shop()`: This is our main function, representing the entire coffee shop operation.\n",
    "\n",
    "3. `orders = ['Espresso', 'Latte', 'Cappuccino', 'Americano']`: These are the coffee orders we need to process.\n",
    "\n",
    "4. `with concurrent.futures.ThreadPoolExecutor(max_workers=2) as barista_team:`: This creates our team of baristas. We have 2 baristas who can work concurrently.\n",
    "\n",
    "5. `future_to_order = {barista_team.submit(make_coffee, order): order for order in orders}`: This is where we submit all our orders to the barista team. Each submission returns a Future object, which is like an order ticket.\n",
    "\n",
    "6. `for future in concurrent.futures.as_completed(future_to_order):`: This is how we check for completed orders. As each order is completed, we process it.\n",
    "\n",
    "7. `result = future.result()`: This is where we 'collect' the finished coffee. If there was a problem making the coffee, an exception would be raised here.\n",
    "\n",
    "The internal flow works like this:\n",
    "\n",
    "1. The cashier (main thread) takes all orders and submits them to the barista team (ThreadPoolExecutor).\n",
    "2. Each order becomes a Future object, which is like a ticket for that order.\n",
    "3. The baristas (worker threads) start working on the orders concurrently.\n",
    "4. The cashier can do other work while waiting for the orders to be completed.\n",
    "5. As each order is completed, the cashier checks the ticket (Future) and announces that the coffee is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Asynchronous Programming with Futures\n",
    "\n",
    "Futures can be combined with asyncio to create powerful asynchronous programs. This is particularly useful for I/O-bound tasks that involve waiting for external resources.\n",
    "\n",
    "Let's create an example that simulates fetching data from multiple APIs concurrently using asyncio and futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "async def fetch_data(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def fetch_all(urls):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            task = asyncio.create_task(fetch_data(session, url))\n",
    "            tasks.append(task)\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "async def main():\n",
    "    urls = [\n",
    "        'https://api.github.com/events',\n",
    "        'https://api.github.com/emojis',\n",
    "        'https://api.github.com/meta'\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = await fetch_all(urls)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"API {i+1} returned {len(result)} items\")\n",
    "    \n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this code and explain each part:\n",
    "\n",
    "1. `async def fetch_data(session, url):` \n",
    "   This asynchronous function fetches data from a given URL using an aiohttp session. It returns the JSON response.\n",
    "\n",
    "2. `async def fetch_all(urls):`\n",
    "   This function creates a single aiohttp session and then creates tasks for fetching data from all URLs concurrently.\n",
    "   - `async with aiohttp.ClientSession() as session:` creates a session that will be used for all requests.\n",
    "   - `task = asyncio.create_task(fetch_data(session, url))` creates a task (which is a wrapper around a coroutine) for each URL.\n",
    "   - `results = await asyncio.gather(*tasks)` waits for all tasks to complete and collects their results.\n",
    "\n",
    "3. `async def main():`\n",
    "   This is our main function that orchestrates the entire process.\n",
    "   - We define a list of URLs to fetch data from.\n",
    "   - We call `fetch_all(urls)` to fetch data from all URLs concurrently.\n",
    "   - We measure the total time taken and print the results.\n",
    "\n",
    "4. `asyncio.run(main())`\n",
    "   This runs our main coroutine, which is the entry point of our asynchronous program.\n",
    "\n",
    "This example demonstrates how futures (in the form of Tasks in asyncio) can be used to perform multiple I/O operations concurrently, potentially saving a significant amount of time compared to sequential execution.\n",
    "\n",
    "Now, let's extend this example to show how we can handle timeouts and cancellations with futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "async def fetch_data(session, url, timeout):\n",
    "    try:\n",
    "        async with session.get(url, timeout=timeout) as response:\n",
    "            return await response.json()\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Timeout occurred for {url}\")\n",
    "        return None\n",
    "\n",
    "async def fetch_all(urls, timeout):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [asyncio.create_task(fetch_data(session, url, timeout)) for url in urls]\n",
    "        return await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "async def main():\n",
    "    urls = [\n",
    "        'https://api.github.com/events',\n",
    "        'https://api.github.com/emojis',\n",
    "        'https://api.github.com/meta',\n",
    "        'https://api.github.com/non_existent'  # This URL doesn't exist\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = await fetch_all(urls, timeout=2)  # 2 second timeout\n",
    "    end_time = time.time()\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"API {i+1} encountered an error: {result}\")\n",
    "        elif result is None:\n",
    "            print(f\"API {i+1} timed out\")\n",
    "        else:\n",
    "            print(f\"API {i+1} returned {len(result)} items\")\n",
    "    \n",
    "    print(f\"Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this extended example:\n",
    "\n",
    "1. We've added a `timeout` parameter to `fetch_data` and `fetch_all` functions.\n",
    "\n",
    "2. In `fetch_data`, we now catch `asyncio.TimeoutError` and return `None` if a timeout occurs.\n",
    "\n",
    "3. In `fetch_all`, we use `return_exceptions=True` in `asyncio.gather()`. This means that if any task raises an exception, it will be returned in the results instead of being raised immediately.\n",
    "\n",
    "4. In `main`, we've added a non-existent URL to demonstrate error handling.\n",
    "\n",
    "5. When processing results, we now check for exceptions and timeouts:\n",
    "   - If the result is an instance of `Exception`, we print an error message.\n",
    "   - If the result is `None`, we know it timed out.\n",
    "   - Otherwise, we print the number of items returned.\n",
    "\n",
    "This example shows how futures can be used to handle concurrent operations with timeouts and error handling, which is crucial in real-world applications where network operations can fail or take too long.\n",
    "\n",
    "The use of futures (via asyncio tasks) allows us to:\n",
    "- Perform multiple I/O operations concurrently\n",
    "- Set timeouts for operations\n",
    "- Handle errors gracefully\n",
    "- Collect results as they become available\n",
    "\n",
    "These capabilities make futures a powerful tool for building efficient and robust asynchronous applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Use Cases and Best Practices\n",
    "\n",
    "### 6.1 Combining Futures with Context Managers\n",
    "\n",
    "Futures can be combined with context managers to ensure proper resource management. This is particularly useful when working with resources that need to be explicitly closed or released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import contextlib\n",
    "import time\n",
    "\n",
    "class DatabaseConnection:\n",
    "    def __init__(self, db_name):\n",
    "        self.db_name = db_name\n",
    "    \n",
    "    def __enter__(self):\n",
    "        print(f\"Connecting to database {self.db_name}\")\n",
    "        time.sleep(1)  # Simulate connection time\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        print(f\"Closing connection to database {self.db_name}\")\n",
    "        time.sleep(0.5)  # Simulate disconnection time\n",
    "    \n",
    "    def query(self, sql):\n",
    "        print(f\"Executing query on {self.db_name}: {sql}\")\n",
    "        time.sleep(2)  # Simulate query execution time\n",
    "        return f\"Result from {self.db_name}\"\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def get_db_connection(db_name):\n",
    "    with DatabaseConnection(db_name) as conn:\n",
    "        yield conn\n",
    "\n",
    "def run_query(db_name, sql):\n",
    "    with get_db_connection(db_name) as conn:\n",
    "        return conn.query(sql)\n",
    "\n",
    "def main():\n",
    "    databases = ['DB1', 'DB2', 'DB3']\n",
    "    sql = \"SELECT * FROM users\"\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [executor.submit(run_query, db, sql) for db in databases]\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            print(f\"Got result: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "1. We define a `DatabaseConnection` class that simulates a database connection. It uses the context manager protocol (`__enter__` and `__exit__` methods) to manage the connection lifecycle.\n",
    "\n",
    "2. We create a `get_db_connection` context manager function using `@contextlib.contextmanager`. This allows us to use the `with` statement with our `DatabaseConnection`.\n",
    "\n",
    "3. The `run_query` function uses the `get_db_connection` context manager to ensure that the database connection is properly closed after the query is executed.\n",
    "\n",
    "4. In the `main` function, we use a `ThreadPoolExecutor` to run queries on multiple databases concurrently.\n",
    "\n",
    "This pattern ensures that resources (in this case, database connections) are properly managed even when used with futures and concurrent execution.\n",
    "\n",
    "### 6.2 Chaining Futures\n",
    "\n",
    "Sometimes, you may want to perform a series of asynchronous operations where each operation depends on the result of the previous one. Futures can be chained to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def fetch_data(data):\n",
    "    print(f\"Fetching data: {data}\")\n",
    "    time.sleep(2)  # Simulate API call\n",
    "    return f\"Fetched: {data}\"\n",
    "\n",
    "def process_data(data):\n",
    "    print(f\"Processing data: {data}\")\n",
    "    time.sleep(1)  # Simulate processing\n",
    "    return f\"Processed: {data}\"\n",
    "\n",
    "def save_data(data):\n",
    "    print(f\"Saving data: {data}\")\n",
    "    time.sleep(1)  # Simulate saving to database\n",
    "    return f\"Saved: {data}\"\n",
    "\n",
    "def chain_operations(executor, initial_data):\n",
    "    fetch_future = executor.submit(fetch_data, initial_data)\n",
    "    \n",
    "    process_future = executor.submit(process_data, fetch_future.result())\n",
    "    \n",
    "    save_future = executor.submit(save_data, process_future.result())\n",
    "    \n",
    "    return save_future\n",
    "\n",
    "def main():\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        future1 = chain_operations(executor, \"Data1\")\n",
    "        future2 = chain_operations(executor, \"Data2\")\n",
    "        \n",
    "        for future in concurrent.futures.as_completed([future1, future2]):\n",
    "            result = future.result()\n",
    "            print(f\"Final result: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "1. We define three functions that simulate different stages of data processing: `fetch_data`, `process_data`, and `save_data`.\n",
    "\n",
    "2. The `chain_operations` function demonstrates how to chain futures:\n",
    "   - It submits the `fetch_data` task and gets a future.\n",
    "   - It then submits the `process_data` task, using `fetch_future.result()` to wait for and get the result of the fetch operation.\n",
    "   - Finally, it submits the `save_data` task, using `process_future.result()` to wait for and get the result of the process operation.\n",
    "\n",
    "3. In the `main` function, we start two chains of operations concurrently and wait for both to complete.\n",
    "\n",
    "This pattern allows you to create pipelines of asynchronous operations while still leveraging the concurrency provided by futures.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use context managers**: Always use futures and executors with context managers (`with` statements) to ensure proper cleanup.\n",
    "\n",
    "2. **Set appropriate timeouts**: When waiting for futures to complete, consider setting timeouts to avoid indefinite waits.\n",
    "\n",
    "3. **Handle exceptions**: Always handle potential exceptions when calling `future.result()` to prevent unhandled exceptions from crashing your program.\n",
    "\n",
    "4. **Choose the right executor**: Use `ThreadPoolExecutor` for I/O-bound tasks and `ProcessPoolExecutor` for CPU-bound tasks.\n",
    "\n",
    "5. **Limit the number of workers**: Set an appropriate `max_workers` value to avoid overwhelming system resources.\n",
    "\n",
    "6. **Use `as_completed` for dynamic results**: When you want to process results as soon as they're available, use `concurrent.futures.as_completed()`.\n",
    "\n",
    "7. **Consider using `asyncio` for complex asynchronous logic**: For more complex asynchronous patterns, consider using `asyncio` in combination with futures.\n",
    "\n",
    "By following these practices and understanding these patterns, you can effectively use futures to create efficient and robust concurrent programs in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Error Handling and Cancellation\n",
    "\n",
    "### 7.1 Detailed Error Handling\n",
    "\n",
    "When working with futures, it's crucial to handle errors properly. Let's look at a more comprehensive example of error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import random\n",
    "import time\n",
    "\n",
    "class DataFetchError(Exception):\n",
    "    pass\n",
    "\n",
    "class DataProcessError(Exception):\n",
    "    pass\n",
    "\n",
    "def fetch_data(id):\n",
    "    print(f\"Fetching data for id {id}\")\n",
    "    time.sleep(random.uniform(0.5, 2))  # Simulate network delay\n",
    "    if random.random() < 0.2:  # 20% chance of failure\n",
    "        raise DataFetchError(f\"Failed to fetch data for id {id}\")\n",
    "    return f\"Data for id {id}\"\n",
    "\n",
    "def process_data(data):\n",
    "    print(f\"Processing {data}\")\n",
    "    time.sleep(random.uniform(0.5, 1.5))  # Simulate processing time\n",
    "    if random.random() < 0.1:  # 10% chance of failure\n",
    "        raise DataProcessError(f\"Failed to process {data}\")\n",
    "    return f\"Processed {data}\"\n",
    "\n",
    "def fetch_and_process(id):\n",
    "    try:\n",
    "        data = fetch_data(id)\n",
    "        return process_data(data)\n",
    "    except DataFetchError as e:\n",
    "        print(f\"Fetch error: {e}\")\n",
    "        return None\n",
    "    except DataProcessError as e:\n",
    "        print(f\"Process error: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    ids = range(1, 11)  # Process 10 items\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_id = {executor.submit(fetch_and_process, id): id for id in ids}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_id):\n",
    "            id = future_to_id[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    print(f\"Successfully processed id {id}: {result}\")\n",
    "                else:\n",
    "                    print(f\"Failed to process id {id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for id {id}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "1. We define custom exceptions `DataFetchError` and `DataProcessError` for specific error cases.\n",
    "\n",
    "2. The `fetch_data` and `process_data` functions simulate network and processing operations that can fail.\n",
    "\n",
    "3. `fetch_and_process` combines these operations and handles their specific exceptions.\n",
    "\n",
    "4. In the `main` function, we use a ThreadPoolExecutor to process multiple items concurrently.\n",
    "\n",
    "5. We use `as_completed` to handle results as they become available, and we catch any unexpected exceptions that might occur when calling `future.result()`.\n",
    "\n",
    "This approach allows us to handle different types of errors at different levels, providing more granular control over error handling and reporting.\n",
    "\n",
    "### 7.2 Cancellation and Timeouts\n",
    "\n",
    "Futures also support cancellation and timeouts, which are crucial for managing long-running or potentially hanging tasks. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def long_running_task(n):\n",
    "    print(f\"Starting long running task {n}\")\n",
    "    try:\n",
    "        time.sleep(10)  # Simulate a long-running operation\n",
    "        return f\"Task {n} completed\"\n",
    "    except concurrent.futures.CancelledError:\n",
    "        print(f\"Task {n} was cancelled\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(long_running_task, i) for i in range(5)]\n",
    "        \n",
    "        # Wait for the first task to complete or 3 seconds to pass\n",
    "        try:\n",
    "            done, not_done = concurrent.futures.wait(futures, timeout=3, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "            \n",
    "            for future in done:\n",
    "                print(future.result())\n",
    "            \n",
    "            # Cancel the tasks that didn't complete\n",
    "            for future in not_done:\n",
    "                future.cancel()\n",
    "            \n",
    "            # Wait for the cancelled tasks to finish\n",
    "            concurrent.futures.wait(not_done, return_when=concurrent.futures.ALL_COMPLETED)\n",
    "        \n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(\"Timeout occurred\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "1. We define a `long_running_task` that simulates a task taking 10 seconds to complete. It's designed to catch `CancelledError` to demonstrate proper cancellation handling.\n",
    "\n",
    "2. In `main`, we submit 5 tasks to a ThreadPoolExecutor.\n",
    "\n",
    "3. We use `concurrent.futures.wait` with a timeout of 3 seconds and `return_when=FIRST_COMPLETED`. This means it will return as soon as either a task completes or 3 seconds pass.\n",
    "\n",
    "4. After the wait, we process any completed tasks and cancel the rest.\n",
    "\n",
    "5. We then wait for the cancelled tasks to finish (which should be quick as they're cancelled).\n",
    "\n",
    "6. We also catch `TimeoutError` in case the initial wait times out before any task completes.\n",
    "\n",
    "This pattern is useful for scenarios where you want to limit the total time spent on a set of tasks, or when you want to cancel remaining tasks once you have a sufficient number of results.\n",
    "\n",
    "## 8. Combining Futures with Other Python Features\n",
    "\n",
    "### 8.1 Futures with Generators\n",
    "\n",
    "Futures can be combined with generators to create powerful data processing pipelines. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "def data_generator(n):\n",
    "    for i in range(n):\n",
    "        yield f\"Data chunk {i}\"\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Simulate processing\n",
    "    return f\"Processed {chunk} (result: {random.randint(1, 100)})\"\n",
    "\n",
    "def process_data_pipeline(data, max_workers=3):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit the first batch of tasks\n",
    "        futures = {executor.submit(process_chunk, next(data)) for _ in range(max_workers)}\n",
    "        \n",
    "        while futures:\n",
    "            # Wait for the next completed future\n",
    "            done, futures = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "            \n",
    "            for future in done:\n",
    "                yield future.result()\n",
    "            \n",
    "            # Submit new tasks to replace the completed ones\n",
    "            for _ in done:\n",
    "                try:\n",
    "                    futures.add(executor.submit(process_chunk, next(data)))\n",
    "                except StopIteration:\n",
    "                    # No more data to process\n",
    "                    pass\n",
    "\n",
    "def main():\n",
    "    data = data_generator(20)  # Generate 20 chunks of data\n",
    "    for result in process_data_pipeline(data):\n",
    "        print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "1. We define a `data_generator` that yields chunks of data.\n",
    "\n",
    "2. The `process_chunk` function simulates processing a single chunk of data.\n",
    "\n",
    "3. `process_data_pipeline` is the core of our pipeline. It:\n",
    "   - Maintains a pool of futures, keeping `max_workers` tasks running at all times.\n",
    "   - Yields results as they become available.\n",
    "   - Submits new tasks to replace completed ones until all data is processed.\n",
    "\n",
    "4. The `main` function demonstrates how to use this pipeline, printing results as they're produced.\n",
    "\n",
    "This pattern allows for efficient processing of streaming data, maintaining a constant level of concurrency while yielding results as soon as they're available.\n",
    "\n",
    "These examples demonstrate more advanced uses of futures, including detailed error handling, cancellation and timeout management, and combining futures with other Python features like generators. They showcase the flexibility and power of futures in creating efficient and robust concurrent programs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
